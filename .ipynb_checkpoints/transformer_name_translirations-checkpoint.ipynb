{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specialized-nebraska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "2021-03-09 16:08:47.015184\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import random\n",
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "approximate-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254928\n",
      "87564\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "with open('data/names.csv', 'r',  encoding=\"utf-8\")as f:\n",
    "    string = \"\"\n",
    "\n",
    "    \n",
    "    line = f.readline()\n",
    "\n",
    "    \n",
    "    \n",
    "    while line:\n",
    "        \n",
    "        if len(line) > 2 and line[-2] == ',':\n",
    "            line = line[:-2] + '\\n'\n",
    "        if len(line) > 2 and line[-2] == '.':\n",
    "            line = line[:-2] + '\\n'\n",
    "        if '’' in line:\n",
    "            line = line.replace(\"’\", \"\\'\")\n",
    "        if '׳' in line:\n",
    "            line = line.replace(\"׳\", \"\\'\")\n",
    "        if 'ʼ' in line:\n",
    "            line = line.replace('ʼ', \"\")\n",
    "        if 'ʾ' in line:\n",
    "            line = line.replace('ʾ', \"\")\n",
    "        if 'ʹ' in line:\n",
    "            line = line.replace('ʹ', \"\")\n",
    "        if 'ʿ' in line:\n",
    "            line = line.replace('ʿ', \"\")\n",
    "        if 'ӏ' in line:\n",
    "            line = line.replace('ӏ', \"l\")\n",
    "        if '–' in line:\n",
    "            line = line.replace('–', \"-\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if \",,\" in line:\n",
    "            # print(line)\n",
    "            \n",
    "            line = line.replace(\",,\", \",\")\n",
    "        if '\\'' in line:\n",
    "            line = line.replace('\\'', '\\\\\\'')\n",
    "        if '\\\"' in line:\n",
    "            line = line.replace('\\\"', '\\\\\\\"')\n",
    "        if '(' in line or ')' in line:\n",
    "            line = \"\"\n",
    "        if line == 'hebrew,english\\n':\n",
    "            # print(index)\n",
    "            line= \"\"\n",
    "        string += line\n",
    "        line = f.readline()\n",
    "\n",
    "\n",
    "with open('data/names_after_processing.csv', 'w',  encoding=\"utf-8\") as fi:\n",
    "    fi.write(string)\n",
    "\n",
    "df = pd.read_csv(\"data/names_after_processing.csv\", names=['hebrew', 'english'],header=None)\n",
    "# print(len(df))\n",
    "wierd_charachters =  ['װ','í', 'á', 'ó', 'č',  'š', 'ş',  'ł',  'ø', 'é', 'ő', 'î', 'è', 'ć', 'ž', 'ö', 'ň', 'ẓ', 'ü', 'ť', 'ý', 'ã', 'ë', 'ī', 'ḥ', 'ă', 'ñ', 'ä', '־', 'ʻ', 'ğ', 'ï', 'ģ', 'ń', 'ç', 'ū', 'ú', 'ļ', 'ṭ', 'å', 'ḳ', '̈', '1', 'ĥ', 'ð', 'ě', 'ō', 'ô', 'ė', 'ò', 'ř', 'ț',  'ą', 'ß', 'đ', '\\xa0', ':', 'æ', 'ê', 'â', 'ı', 'ù', 'à', 'ś', '︠', '︡',  'ż',  'ţ', 'þ', 'ľ',  'ā', '0', 'ų', 'ĭ', '8', 'ē', 'ņ', 'ÿ', 'ַ', 'θ', 'ε', 'δ', 'ω', 'ρ', 'α', 'ό', 'π', 'υ', 'λ', 'ς', '3', 'õ', 'م', 'ا', 'ل', 'ي', 'ن', '،', 'ׂ', '$', '5', '2', '室', '田', '一', '雄', '′', 'س', 'ف', 'ầ', 'ů', '͡', 'ŏ', 'ả', 'ź', 'ì', '̣', 'ب', 'ر', 'و', 'ױ', '_', 'ə', 'û', 'ŭ', 'د', '9', 'ṣ', 'ḵ', '6', 'أ', 'ك', 'گ', 'ی', '!', 'ت',  'ď', '4', 'ŵ', 'ķ', 'ش', 'ứ', '7', 'œ', 'ũ', 'ű', 'ụ', 'ه', 'ạ', 'ح', 'ذ', 'τ', 'ά', 'ι', 'ṇ','ę', '׳', 'ׂ', '̇', '’', '.', '־', '/', 'ִ', 'ְ', 'ּ', 'ș', 'ʼ', 'ֶ', 'ַ', 'ʾ', '室', '田', '一', '雄', 'ײ', '‘', 'ʹ', 'ׁ', ':', '[', 'س', 'ي', 'ف', 'ر', 'ت', 'ֹ', 'ا', 'ن', 'ṿ', 'ָ', 'ֵ', '!', 'ʿ', '_', 'أ', 'ب', 'و', '`', '1', '0', '8', 'ش', 'ل', 'κ', 'ӏ', 'ׄ', 'م', 'د', '2', 'τ', 'ά', 'ι', 'θ', 'ε', 'ο', 'δ', 'ω', 'ρ', 'α', 'ό', 'π', 'υ', 'λ', 'ς', '\\xa0', '،', '״', 'ױ', '5', 'ك']\n",
    "\n",
    "df.drop(df[(df['english'] == 'i')|\n",
    "(df['english'] == 'ii')|(df['english'] == 'iii')\n",
    "|(df['english'] == 'iv')|(df['english'] == 'v')\n",
    "|(df['english'] == 'vi')|(df['english'] == 'vii')\n",
    "|(df['english'] == 'viii')|(df['english'] == 'ix')\n",
    "|(df['english'] == 'x')|(df['english'] == 'xi')\n",
    "|(df['english'] == 'xii')|(df['english'] == 'xiii')\n",
    "|(df['english'] == 'prince')|(df['english'] == 'princess')\n",
    "|(df['english'] == 'dutchess')|(df['english'] == 'duke')\n",
    "|(df['english'] == 'mr')|(df['english'] == 'ms')|(df['english'] == 'mrs')\n",
    "|(df['english'] == 'the')|(df['english'] == 'for')|(df['english'] == 'of')\n",
    "|(df['english'] == 'king')|(df['english'] == 'queen')|(df['english'] == 'is')\n",
    "|(df['english'].str.contains('^[a-z][.]', regex =True))|(df['english'].str.contains('^[א-ת][.]', regex =True))\n",
    "|(df['english'].str.contains('^[a-z]$', regex =True))|(df['english'].str.contains('^[א-ת]$', regex =True))\n",
    "].index, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "for char in wierd_charachters:\n",
    "    df.drop(df[(df['english'].str.contains(char, regex=False))|(df['hebrew'].str.contains(char, regex=False)) ].index, inplace=True)\n",
    "    \n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "df.drop(df.loc[df['english'].str.contains('[א-ת]', regex=True)].index, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv('names_final.csv', index = False)\n",
    "print(len(df))\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_validate_test_split(dataframe, train_percent=.9, validate_percent=0.0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(dataframe.index)\n",
    "   \n",
    "    m = len(dataframe.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    print(max(perm))\n",
    "    \n",
    "    train = dataframe.iloc[perm[:train_end]]\n",
    "    validate = dataframe.iloc[perm[train_end:validate_end]]\n",
    "    test = dataframe.iloc[perm[validate_end:]]\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "macro-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87563\n",
      "Train size 78262, Dev size 0, Test size 8710 vocab english size 30 vocab hebrew size 34\n"
     ]
    }
   ],
   "source": [
    "MAX_LABEL_LENGTH = 12\n",
    "class Vocab:\n",
    "    def __init__(self, is_hebrew):\n",
    "        self.char2id = {}\n",
    "        self.id2char = {}\n",
    "        if is_hebrew:\n",
    "            self.n_chars = 2        \n",
    "            self.char2id['^'] = 0\n",
    "            self.char2id['$'] = 1\n",
    "            self.id2char[0] = '^'\n",
    "            self.id2char[1] = '$'\n",
    "        else:\n",
    "            self.n_chars = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def index_text(self, text):\n",
    "        indexes = [self.index_char(c) for c in text]\n",
    "        return indexes\n",
    "    \n",
    "    def index_char(self, c):\n",
    "        if c not in self.char2id:\n",
    "            self.char2id[c] = self.n_chars\n",
    "            self.id2char[self.n_chars] = c\n",
    "            self.n_chars += 1\n",
    "        return self.char2id[c]\n",
    "            \n",
    "            \n",
    "def load_data(data, vocab_english, vocab_hebrew):\n",
    "    data_sequences = []\n",
    "    max_word = 0\n",
    "    num_more_than_20 = 0 \n",
    "    max_word_value = None\n",
    "    for text in data.itertuples():\n",
    "        indexes_english = vocab_english.index_text(text.english)\n",
    "        indexes_hebrew = vocab_hebrew.index_text(text.hebrew)\n",
    "        if len(indexes_hebrew) > max_word:\n",
    "            max_word = len(indexes_hebrew)\n",
    "            max_word_value = indexes_hebrew\n",
    "        if len(indexes_hebrew) <= MAX_LABEL_LENGTH:\n",
    "#             for i in range(len(indexes_hebrew),MAX_LABEL_LENGTH, 1):\n",
    "            indexes_hebrew.append(1)\n",
    "            data_sequences.append((indexes_english, indexes_hebrew))\n",
    "\n",
    "    return data_sequences\n",
    "\n",
    "vocab_english = Vocab(False)\n",
    "vocab_hebrew = Vocab(True)\n",
    "train_set, validation_set, test_set = train_validate_test_split(dataframe=df)\n",
    "\n",
    "train_data = load_data(train_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "dev_data = load_data(validation_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "test_data = load_data(test_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Train size {len(train_data)}, Dev size {len(dev_data)}, Test size {len(test_data)} vocab english size {vocab_english.n_chars} vocab hebrew size {vocab_hebrew.n_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "floppy-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([12, 2, 12, 14, 0, 4, 1], [13, 3, 15, 2, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "small_train_data = train_data[:8000]\n",
    "len(small_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "freelance-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=160):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x * math.sqrt(self.d_model)\n",
    "            seq_len = x.size(1)\n",
    "            pe = self.pe[:, :seq_len]\n",
    "            x = x + pe\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "considerable-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_mask(size_of_mask):\n",
    "        mask = (torch.triu(torch.ones(size_of_mask, size_of_mask)) == 0)\n",
    "\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "enhanced-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_model(torch.nn.Module):\n",
    "    def __init__(self, character_amount_english,character_amount_hebrew, embedding_size, activation=\"relu\", nhead=8, dim_feedforward=1024, num_encoder_layers=2, num_decoder_layers=2):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.embedding_english = torch.nn.Embedding(character_amount_english, embedding_size).cuda()\n",
    "        self.embedding_hebrew = torch.nn.Embedding(character_amount_hebrew, embedding_size).cuda()\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoder(embedding_size)\n",
    "\n",
    "        \n",
    "        self.transformer = torch.nn.Transformer(d_model=embedding_size, num_encoder_layers=num_encoder_layers,nhead=nhead,dim_feedforward=dim_feedforward, num_decoder_layers=num_decoder_layers).cuda()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(embedding_size, character_amount_hebrew)\n",
    "   \n",
    "     \n",
    "\n",
    "    \n",
    "    def generate_mask(self, size_of_mask):\n",
    "         \n",
    "        return self.transformer.generate_square_subsequent_mask(size_of_mask)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, src, trg, mask):\n",
    "\n",
    "        embedded_english = self.embedding_english(src)\n",
    "        embedded_hebrew = self.embedding_hebrew(trg)\n",
    "        embedded_english = self.positional_encoding(embedded_english)\n",
    "\n",
    "        embedded_hebrew = self.positional_encoding(embedded_hebrew)\n",
    "\n",
    "        out = self.transformer(embedded_english, embedded_hebrew, tgt_mask=mask, src_mask=None).cuda()\n",
    "        out = self.linear(out)\n",
    "\n",
    "        \n",
    "        out = out.reshape(out.shape[0],out.shape[2])\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "satellite-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = Attention_model(vocab_english.n_chars, vocab_hebrew.n_chars, 320).cuda()\n",
    "optimizer_transformer = torch.optim.Adam([\n",
    "        dict(params=model_transformer.parameters(), lr=0.0001),\n",
    "    ])\n",
    "\n",
    "criterion_transformer = torch.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "minus-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(n_epochs=4, print_every=5000):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(1, n_epochs + 1):\n",
    "        \n",
    "\n",
    "        for counter, (english_name, hebrew_name) in enumerate(small_train_data):\n",
    "\n",
    "            optimizer_transformer.zero_grad()\n",
    "\n",
    "            english_name_tensor = torch.LongTensor(english_name).cuda()\n",
    "            hebrew_name_temp = hebrew_name.copy()\n",
    "\n",
    "            hebrew_name_temp.insert(0,0)\n",
    "            hebrew_name_temp.pop()\n",
    "\n",
    "\n",
    "            mask = model_transformer.generate_mask(len(hebrew_name_temp)).cuda()\n",
    "\n",
    "            hebrew_name_tensor = torch.LongTensor(hebrew_name_temp).cuda()\n",
    "\n",
    "            output = model_transformer(english_name_tensor.reshape(-1,1), hebrew_name_tensor.reshape(-1,1), mask)\n",
    "\n",
    "            indexes = torch.argmax(output, dim=1)\n",
    "\n",
    "            hebrew_name_temp.pop(0)\n",
    "            hebrew_name_temp.append(1)\n",
    "            transliration_loss = 0\n",
    "            for i, row in enumerate(output): \n",
    "\n",
    "                transliration_loss += criterion_transformer(row.reshape(1, len(row)), torch.LongTensor([hebrew_name_temp[i]]).cuda())\n",
    "\n",
    "            transliration_loss.backward()\n",
    "            \n",
    "\n",
    "            optimizer_transformer.step()\n",
    "            \n",
    "\n",
    "            loss += (transliration_loss.item() / 12)\n",
    "\n",
    "            if counter % print_every == 0 and counter != 0:\n",
    "                loss = loss / print_every\n",
    "                print('Epoch %d,/%d, Current Loss = %.4f' % (e, counter,loss))\n",
    "                loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "conventional-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-09 21:51:18.167783\n",
      "Epoch 1,/100, Current Loss = 1.6705\n",
      "Epoch 1,/200, Current Loss = 1.5101\n",
      "Epoch 1,/300, Current Loss = 1.3730\n",
      "Epoch 1,/400, Current Loss = 1.2719\n",
      "Epoch 1,/500, Current Loss = 1.3044\n",
      "Epoch 1,/600, Current Loss = 1.3043\n",
      "Epoch 1,/700, Current Loss = 1.2090\n",
      "Epoch 1,/800, Current Loss = 1.1718\n",
      "Epoch 1,/900, Current Loss = 1.1560\n",
      "Epoch 1,/1000, Current Loss = 1.2242\n",
      "Epoch 1,/1100, Current Loss = 1.0947\n",
      "Epoch 1,/1200, Current Loss = 1.1195\n",
      "Epoch 1,/1300, Current Loss = 1.1210\n",
      "Epoch 1,/1400, Current Loss = 1.0772\n",
      "Epoch 1,/1500, Current Loss = 1.1448\n",
      "Epoch 1,/1600, Current Loss = 1.1107\n",
      "Epoch 1,/1700, Current Loss = 1.1626\n",
      "Epoch 1,/1800, Current Loss = 1.0635\n",
      "Epoch 1,/1900, Current Loss = 1.1807\n",
      "Epoch 1,/2000, Current Loss = 1.1166\n",
      "Epoch 1,/2100, Current Loss = 1.1021\n",
      "Epoch 1,/2200, Current Loss = 1.0476\n",
      "Epoch 1,/2300, Current Loss = 1.1119\n",
      "Epoch 1,/2400, Current Loss = 1.0848\n",
      "Epoch 1,/2500, Current Loss = 1.0634\n",
      "Epoch 1,/2600, Current Loss = 1.0726\n",
      "Epoch 1,/2700, Current Loss = 1.0605\n",
      "Epoch 1,/2800, Current Loss = 1.0459\n",
      "Epoch 1,/2900, Current Loss = 1.1090\n",
      "Epoch 1,/3000, Current Loss = 1.0604\n",
      "Epoch 1,/3100, Current Loss = 1.0591\n",
      "Epoch 1,/3200, Current Loss = 1.1024\n",
      "Epoch 1,/3300, Current Loss = 0.9570\n",
      "Epoch 1,/3400, Current Loss = 1.0303\n",
      "Epoch 1,/3500, Current Loss = 0.8986\n",
      "Epoch 1,/3600, Current Loss = 0.9455\n",
      "Epoch 1,/3700, Current Loss = 1.0841\n",
      "Epoch 1,/3800, Current Loss = 1.0131\n",
      "Epoch 1,/3900, Current Loss = 1.0896\n",
      "Epoch 1,/4000, Current Loss = 1.0004\n",
      "Epoch 1,/4100, Current Loss = 0.9896\n",
      "Epoch 1,/4200, Current Loss = 1.0822\n",
      "Epoch 1,/4300, Current Loss = 1.0727\n",
      "Epoch 1,/4400, Current Loss = 1.0338\n",
      "Epoch 1,/4500, Current Loss = 0.9774\n",
      "Epoch 1,/4600, Current Loss = 0.9819\n",
      "Epoch 1,/4700, Current Loss = 0.9809\n",
      "Epoch 1,/4800, Current Loss = 0.9772\n",
      "Epoch 1,/4900, Current Loss = 1.0925\n",
      "Epoch 1,/5000, Current Loss = 1.0520\n",
      "Epoch 1,/5100, Current Loss = 1.0460\n",
      "Epoch 1,/5200, Current Loss = 0.9268\n",
      "Epoch 1,/5300, Current Loss = 0.9677\n",
      "Epoch 1,/5400, Current Loss = 1.0431\n",
      "Epoch 1,/5500, Current Loss = 0.9979\n",
      "Epoch 1,/5600, Current Loss = 0.9461\n",
      "Epoch 1,/5700, Current Loss = 0.9964\n",
      "Epoch 1,/5800, Current Loss = 0.9830\n",
      "Epoch 1,/5900, Current Loss = 0.9654\n",
      "Epoch 1,/6000, Current Loss = 0.9511\n",
      "Epoch 1,/6100, Current Loss = 1.0466\n",
      "Epoch 1,/6200, Current Loss = 0.9962\n",
      "Epoch 1,/6300, Current Loss = 0.9908\n",
      "Epoch 1,/6400, Current Loss = 1.0288\n",
      "Epoch 1,/6500, Current Loss = 1.0574\n",
      "Epoch 1,/6600, Current Loss = 1.0703\n",
      "Epoch 1,/6700, Current Loss = 1.0285\n",
      "Epoch 1,/6800, Current Loss = 1.0332\n",
      "Epoch 1,/6900, Current Loss = 1.0537\n",
      "Epoch 1,/7000, Current Loss = 0.9777\n",
      "Epoch 1,/7100, Current Loss = 0.9504\n",
      "Epoch 1,/7200, Current Loss = 1.0287\n",
      "Epoch 1,/7300, Current Loss = 0.9920\n",
      "Epoch 1,/7400, Current Loss = 1.0621\n",
      "Epoch 1,/7500, Current Loss = 0.9675\n",
      "Epoch 1,/7600, Current Loss = 0.9427\n",
      "Epoch 1,/7700, Current Loss = 0.9954\n",
      "Epoch 1,/7800, Current Loss = 0.8868\n",
      "Epoch 1,/7900, Current Loss = 0.9760\n",
      "Epoch 2,/100, Current Loss = 1.9288\n",
      "Epoch 2,/200, Current Loss = 1.0420\n",
      "Epoch 2,/300, Current Loss = 1.0138\n",
      "Epoch 2,/400, Current Loss = 0.9388\n",
      "Epoch 2,/500, Current Loss = 1.0277\n",
      "Epoch 2,/600, Current Loss = 1.0375\n",
      "Epoch 2,/700, Current Loss = 0.9525\n",
      "Epoch 2,/800, Current Loss = 0.9168\n",
      "Epoch 2,/900, Current Loss = 0.9319\n",
      "Epoch 2,/1000, Current Loss = 1.0159\n",
      "Epoch 2,/1100, Current Loss = 0.8783\n",
      "Epoch 2,/1200, Current Loss = 0.9159\n",
      "Epoch 2,/1300, Current Loss = 0.9121\n",
      "Epoch 2,/1400, Current Loss = 0.8794\n",
      "Epoch 2,/1500, Current Loss = 0.9972\n",
      "Epoch 2,/1600, Current Loss = 0.9311\n",
      "Epoch 2,/1700, Current Loss = 1.0323\n",
      "Epoch 2,/1800, Current Loss = 0.9174\n",
      "Epoch 2,/1900, Current Loss = 0.9944\n",
      "Epoch 2,/2000, Current Loss = 0.9543\n",
      "Epoch 2,/2100, Current Loss = 0.9676\n",
      "Epoch 2,/2200, Current Loss = 0.9399\n",
      "Epoch 2,/2300, Current Loss = 0.9732\n",
      "Epoch 2,/2400, Current Loss = 0.9593\n",
      "Epoch 2,/2500, Current Loss = 0.9125\n",
      "Epoch 2,/2600, Current Loss = 0.9481\n",
      "Epoch 2,/2700, Current Loss = 0.9321\n",
      "Epoch 2,/2800, Current Loss = 0.9140\n",
      "Epoch 2,/2900, Current Loss = 1.0231\n",
      "Epoch 2,/3000, Current Loss = 0.9705\n",
      "Epoch 2,/3100, Current Loss = 0.9635\n",
      "Epoch 2,/3200, Current Loss = 1.0167\n",
      "Epoch 2,/3300, Current Loss = 0.8568\n",
      "Epoch 2,/3400, Current Loss = 0.9251\n",
      "Epoch 2,/3500, Current Loss = 0.8124\n",
      "Epoch 2,/3600, Current Loss = 0.8596\n",
      "Epoch 2,/3700, Current Loss = 1.0145\n",
      "Epoch 2,/3800, Current Loss = 0.9435\n",
      "Epoch 2,/3900, Current Loss = 0.9962\n",
      "Epoch 2,/4000, Current Loss = 0.9152\n",
      "Epoch 2,/4100, Current Loss = 0.9141\n",
      "Epoch 2,/4200, Current Loss = 0.9840\n",
      "Epoch 2,/4300, Current Loss = 0.9856\n",
      "Epoch 2,/4400, Current Loss = 0.9360\n",
      "Epoch 2,/4500, Current Loss = 0.9069\n",
      "Epoch 2,/4600, Current Loss = 0.8940\n",
      "Epoch 2,/4700, Current Loss = 0.9186\n",
      "Epoch 2,/4800, Current Loss = 0.9047\n",
      "Epoch 2,/4900, Current Loss = 0.9987\n",
      "Epoch 2,/5000, Current Loss = 0.9757\n",
      "Epoch 2,/5100, Current Loss = 0.9729\n",
      "Epoch 2,/5200, Current Loss = 0.8589\n",
      "Epoch 2,/5300, Current Loss = 0.9063\n",
      "Epoch 2,/5400, Current Loss = 0.9533\n",
      "Epoch 2,/5500, Current Loss = 0.9385\n",
      "Epoch 2,/5600, Current Loss = 0.8723\n",
      "Epoch 2,/5700, Current Loss = 0.9282\n",
      "Epoch 2,/5800, Current Loss = 0.9078\n",
      "Epoch 2,/5900, Current Loss = 0.9066\n",
      "Epoch 2,/6000, Current Loss = 0.8616\n",
      "Epoch 2,/6100, Current Loss = 0.9596\n",
      "Epoch 2,/6200, Current Loss = 0.9346\n",
      "Epoch 2,/6300, Current Loss = 0.9058\n",
      "Epoch 2,/6400, Current Loss = 0.9392\n",
      "Epoch 2,/6500, Current Loss = 0.9907\n",
      "Epoch 2,/6600, Current Loss = 0.9923\n",
      "Epoch 2,/6700, Current Loss = 0.9668\n",
      "Epoch 2,/6800, Current Loss = 0.9683\n",
      "Epoch 2,/6900, Current Loss = 0.9820\n",
      "Epoch 2,/7000, Current Loss = 0.9166\n",
      "Epoch 2,/7100, Current Loss = 0.8806\n",
      "Epoch 2,/7200, Current Loss = 0.9505\n",
      "Epoch 2,/7300, Current Loss = 0.9186\n",
      "Epoch 2,/7400, Current Loss = 1.0166\n",
      "Epoch 2,/7500, Current Loss = 0.9375\n",
      "Epoch 2,/7600, Current Loss = 0.8682\n",
      "Epoch 2,/7700, Current Loss = 0.9595\n",
      "Epoch 2,/7800, Current Loss = 0.8283\n",
      "Epoch 2,/7900, Current Loss = 0.9259\n",
      "Epoch 3,/100, Current Loss = 1.8281\n",
      "Epoch 3,/200, Current Loss = 0.9817\n",
      "Epoch 3,/300, Current Loss = 0.9374\n",
      "Epoch 3,/400, Current Loss = 0.8962\n",
      "Epoch 3,/500, Current Loss = 0.9701\n",
      "Epoch 3,/600, Current Loss = 0.9919\n",
      "Epoch 3,/700, Current Loss = 0.8902\n",
      "Epoch 3,/800, Current Loss = 0.8713\n",
      "Epoch 3,/900, Current Loss = 0.8960\n",
      "Epoch 3,/1000, Current Loss = 0.9541\n",
      "Epoch 3,/1100, Current Loss = 0.8406\n",
      "Epoch 3,/1200, Current Loss = 0.8412\n",
      "Epoch 3,/1300, Current Loss = 0.8620\n",
      "Epoch 3,/1400, Current Loss = 0.8133\n",
      "Epoch 3,/1500, Current Loss = 0.9429\n",
      "Epoch 3,/1600, Current Loss = 0.8856\n",
      "Epoch 3,/1700, Current Loss = 0.9848\n",
      "Epoch 3,/1800, Current Loss = 0.8540\n",
      "Epoch 3,/1900, Current Loss = 0.9441\n",
      "Epoch 3,/2000, Current Loss = 0.9319\n",
      "Epoch 3,/2100, Current Loss = 0.9026\n",
      "Epoch 3,/2200, Current Loss = 0.8883\n",
      "Epoch 3,/2300, Current Loss = 0.9219\n",
      "Epoch 3,/2400, Current Loss = 0.9360\n",
      "Epoch 3,/2500, Current Loss = 0.8757\n",
      "Epoch 3,/2600, Current Loss = 0.9136\n",
      "Epoch 3,/2700, Current Loss = 0.8943\n",
      "Epoch 3,/2800, Current Loss = 0.8681\n",
      "Epoch 3,/2900, Current Loss = 0.9660\n",
      "Epoch 3,/3000, Current Loss = 0.9203\n",
      "Epoch 3,/3100, Current Loss = 0.9092\n",
      "Epoch 3,/3200, Current Loss = 0.9707\n",
      "Epoch 3,/3300, Current Loss = 0.8172\n",
      "Epoch 3,/3400, Current Loss = 0.8882\n",
      "Epoch 3,/3500, Current Loss = 0.7519\n",
      "Epoch 3,/3600, Current Loss = 0.8275\n",
      "Epoch 3,/3700, Current Loss = 0.9619\n",
      "Epoch 3,/3800, Current Loss = 0.9212\n",
      "Epoch 3,/3900, Current Loss = 0.9201\n",
      "Epoch 3,/4000, Current Loss = 0.8980\n",
      "Epoch 3,/4100, Current Loss = 0.8558\n",
      "Epoch 3,/4200, Current Loss = 0.9535\n",
      "Epoch 3,/4300, Current Loss = 0.9351\n",
      "Epoch 3,/4400, Current Loss = 0.9004\n",
      "Epoch 3,/4500, Current Loss = 0.8574\n",
      "Epoch 3,/4600, Current Loss = 0.8359\n",
      "Epoch 3,/4700, Current Loss = 0.8486\n",
      "Epoch 3,/4800, Current Loss = 0.8623\n",
      "Epoch 3,/4900, Current Loss = 0.9735\n",
      "Epoch 3,/5000, Current Loss = 0.9325\n",
      "Epoch 3,/5100, Current Loss = 0.9234\n",
      "Epoch 3,/5200, Current Loss = 0.8483\n",
      "Epoch 3,/5300, Current Loss = 0.8594\n",
      "Epoch 3,/5400, Current Loss = 0.9276\n",
      "Epoch 3,/5500, Current Loss = 0.8999\n",
      "Epoch 3,/5600, Current Loss = 0.8280\n",
      "Epoch 3,/5700, Current Loss = 0.8793\n",
      "Epoch 3,/5800, Current Loss = 0.8624\n",
      "Epoch 3,/5900, Current Loss = 0.8574\n",
      "Epoch 3,/6000, Current Loss = 0.8416\n",
      "Epoch 3,/6100, Current Loss = 0.9350\n",
      "Epoch 3,/6200, Current Loss = 0.8943\n",
      "Epoch 3,/6300, Current Loss = 0.8939\n",
      "Epoch 3,/6400, Current Loss = 0.9470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3,/6500, Current Loss = 0.9535\n",
      "Epoch 3,/6600, Current Loss = 0.9235\n",
      "Epoch 3,/6700, Current Loss = 0.9365\n",
      "Epoch 3,/6800, Current Loss = 0.9427\n",
      "Epoch 3,/6900, Current Loss = 0.9311\n",
      "Epoch 3,/7000, Current Loss = 0.9086\n",
      "Epoch 3,/7100, Current Loss = 0.8762\n",
      "Epoch 3,/7200, Current Loss = 0.9180\n",
      "Epoch 3,/7300, Current Loss = 0.8777\n",
      "Epoch 3,/7400, Current Loss = 0.9560\n",
      "Epoch 3,/7500, Current Loss = 0.8907\n",
      "Epoch 3,/7600, Current Loss = 0.8480\n",
      "Epoch 3,/7700, Current Loss = 0.8789\n",
      "Epoch 3,/7800, Current Loss = 0.8031\n",
      "Epoch 3,/7900, Current Loss = 0.8870\n",
      "Epoch 4,/100, Current Loss = 1.7468\n",
      "Epoch 4,/200, Current Loss = 0.9485\n",
      "Epoch 4,/300, Current Loss = 0.9282\n",
      "Epoch 4,/400, Current Loss = 0.8507\n",
      "Epoch 4,/500, Current Loss = 0.9165\n",
      "Epoch 4,/600, Current Loss = 0.9492\n",
      "Epoch 4,/700, Current Loss = 0.8734\n",
      "Epoch 4,/800, Current Loss = 0.8355\n",
      "Epoch 4,/900, Current Loss = 0.8619\n",
      "Epoch 4,/1000, Current Loss = 0.9262\n",
      "Epoch 4,/1100, Current Loss = 0.8043\n",
      "Epoch 4,/1200, Current Loss = 0.8483\n",
      "Epoch 4,/1300, Current Loss = 0.8338\n",
      "Epoch 4,/1400, Current Loss = 0.7926\n",
      "Epoch 4,/1500, Current Loss = 0.9341\n",
      "Epoch 4,/1600, Current Loss = 0.8685\n",
      "Epoch 4,/1700, Current Loss = 0.9604\n",
      "Epoch 4,/1800, Current Loss = 0.8223\n",
      "Epoch 4,/1900, Current Loss = 0.9174\n",
      "Epoch 4,/2000, Current Loss = 0.8883\n",
      "Epoch 4,/2100, Current Loss = 0.8675\n",
      "Epoch 4,/2200, Current Loss = 0.8774\n",
      "Epoch 4,/2300, Current Loss = 0.8978\n",
      "Epoch 4,/2400, Current Loss = 0.8885\n",
      "Epoch 4,/2500, Current Loss = 0.8301\n",
      "Epoch 4,/2600, Current Loss = 0.8876\n",
      "Epoch 4,/2700, Current Loss = 0.8601\n",
      "Epoch 4,/2800, Current Loss = 0.8479\n",
      "Epoch 4,/2900, Current Loss = 0.9517\n",
      "Epoch 4,/3000, Current Loss = 0.8907\n",
      "Epoch 4,/3100, Current Loss = 0.8847\n",
      "Epoch 4,/3200, Current Loss = 0.9543\n",
      "Epoch 4,/3300, Current Loss = 0.7779\n",
      "Epoch 4,/3400, Current Loss = 0.8596\n",
      "Epoch 4,/3500, Current Loss = 0.7294\n",
      "Epoch 4,/3600, Current Loss = 0.7876\n",
      "Epoch 4,/3700, Current Loss = 0.9347\n",
      "Epoch 4,/3800, Current Loss = 0.8776\n",
      "Epoch 4,/3900, Current Loss = 0.9128\n",
      "Epoch 4,/4000, Current Loss = 0.8388\n",
      "Epoch 4,/4100, Current Loss = 0.8137\n",
      "Epoch 4,/4200, Current Loss = 0.9243\n",
      "Epoch 4,/4300, Current Loss = 0.9114\n",
      "Epoch 4,/4400, Current Loss = 0.8860\n",
      "Epoch 4,/4500, Current Loss = 0.8300\n",
      "Epoch 4,/4600, Current Loss = 0.8274\n",
      "Epoch 4,/4700, Current Loss = 0.8335\n",
      "Epoch 4,/4800, Current Loss = 0.8186\n",
      "Epoch 4,/4900, Current Loss = 0.9310\n",
      "Epoch 4,/5000, Current Loss = 0.9278\n",
      "Epoch 4,/5100, Current Loss = 0.9091\n",
      "Epoch 4,/5200, Current Loss = 0.8108\n",
      "Epoch 4,/5300, Current Loss = 0.8408\n",
      "Epoch 4,/5400, Current Loss = 0.8737\n",
      "Epoch 4,/5500, Current Loss = 0.8838\n",
      "Epoch 4,/5600, Current Loss = 0.8129\n",
      "Epoch 4,/5700, Current Loss = 0.8518\n",
      "Epoch 4,/5800, Current Loss = 0.8286\n",
      "Epoch 4,/5900, Current Loss = 0.8516\n",
      "Epoch 4,/6000, Current Loss = 0.8069\n",
      "Epoch 4,/6100, Current Loss = 0.8976\n",
      "Epoch 4,/6200, Current Loss = 0.8365\n",
      "Epoch 4,/6300, Current Loss = 0.8699\n",
      "Epoch 4,/6400, Current Loss = 0.8950\n",
      "Epoch 4,/6500, Current Loss = 0.9239\n",
      "Epoch 4,/6600, Current Loss = 0.9141\n",
      "Epoch 4,/6700, Current Loss = 0.9005\n",
      "Epoch 4,/6800, Current Loss = 0.8940\n",
      "Epoch 4,/6900, Current Loss = 0.9263\n",
      "Epoch 4,/7000, Current Loss = 0.8276\n",
      "Epoch 4,/7100, Current Loss = 0.8170\n",
      "Epoch 4,/7200, Current Loss = 0.8841\n",
      "Epoch 4,/7300, Current Loss = 0.8506\n",
      "Epoch 4,/7400, Current Loss = 0.9360\n",
      "Epoch 4,/7500, Current Loss = 0.8706\n",
      "Epoch 4,/7600, Current Loss = 0.8216\n",
      "Epoch 4,/7700, Current Loss = 0.8547\n",
      "Epoch 4,/7800, Current Loss = 0.7654\n",
      "Epoch 4,/7900, Current Loss = 0.8295\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "train_transformer(print_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "front-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer2(print_every=5000):\n",
    "    accuracy = 0\n",
    "    num_of_letters = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for counter, (english_name, hebrew_name) in enumerate(test_data):\n",
    "            \n",
    "            if(counter == 0):\n",
    "                continue\n",
    "           \n",
    "\n",
    "            english_tensor = torch.LongTensor(english_name).cuda()\n",
    "            sow = torch.tensor([0]).cuda()\n",
    "            hebrew_tensor = sow\n",
    "            \n",
    "            \n",
    "            guessed_word = []\n",
    "            \n",
    "            for i in range(MAX_LABEL_LENGTH):\n",
    "\n",
    "                output = model_transformer(english_tensor.reshape(-1,1), hebrew_tensor.reshape(-1,1), mask=None)\n",
    "\n",
    "                indexes = torch.argmax(output, dim=1)\n",
    "\n",
    "                hebrew_tensor = torch.cat((sow, indexes))\n",
    "\n",
    "                \n",
    "                hebrew_tensor = hebrew_tensor.reshape(len(hebrew_tensor), 1)\n",
    "\n",
    "            for i in range(min(len(hebrew_name), len(indexes))):\n",
    "\n",
    "                num_of_letters += 1\n",
    "                \n",
    "                if indexes[i].item() == hebrew_name[i]:\n",
    "                    accuracy += 1\n",
    "\n",
    "            for i in range(min(len(hebrew_name), len(indexes)), max(len(hebrew_name), len(indexes))):\n",
    "                num_of_letters += 1        \n",
    "                           \n",
    "            if counter % print_every == 0 and counter != 0:\n",
    "                print(counter)\n",
    "    print(accuracy / num_of_letters)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "understanding-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "0.0934260357293141\n",
      "2021-03-09 22:34:35.376589\n"
     ]
    }
   ],
   "source": [
    "test_transformer2(print_every=500)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-neutral",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
