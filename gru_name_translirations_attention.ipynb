{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "00:59:24\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import random\n",
    "import time\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorrect-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254928\n",
      "87564\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "with open('data/names.csv', 'r',  encoding=\"utf-8\")as f:\n",
    "    string = \"\"\n",
    "\n",
    "    \n",
    "    line = f.readline()\n",
    "\n",
    "    \n",
    "    \n",
    "    while line:\n",
    "        \n",
    "        if len(line) > 2 and line[-2] == ',':\n",
    "            line = line[:-2] + '\\n'\n",
    "        if len(line) > 2 and line[-2] == '.':\n",
    "            line = line[:-2] + '\\n'\n",
    "        if '’' in line:\n",
    "            line = line.replace(\"’\", \"\\'\")\n",
    "        if '׳' in line:\n",
    "            line = line.replace(\"׳\", \"\\'\")\n",
    "        if 'ʼ' in line:\n",
    "            line = line.replace('ʼ', \"\")\n",
    "        if 'ʾ' in line:\n",
    "            line = line.replace('ʾ', \"\")\n",
    "        if 'ʹ' in line:\n",
    "            line = line.replace('ʹ', \"\")\n",
    "        if 'ʿ' in line:\n",
    "            line = line.replace('ʿ', \"\")\n",
    "        if 'ӏ' in line:\n",
    "            line = line.replace('ӏ', \"l\")\n",
    "        if '–' in line:\n",
    "            line = line.replace('–', \"-\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if \",,\" in line:\n",
    "            # print(line)\n",
    "            \n",
    "            line = line.replace(\",,\", \",\")\n",
    "        if '\\'' in line:\n",
    "            line = line.replace('\\'', '\\\\\\'')\n",
    "        if '\\\"' in line:\n",
    "            line = line.replace('\\\"', '\\\\\\\"')\n",
    "        if '(' in line or ')' in line:\n",
    "            line = \"\"\n",
    "        if line == 'hebrew,english\\n':\n",
    "            # print(index)\n",
    "            line= \"\"\n",
    "        string += line\n",
    "        line = f.readline()\n",
    "\n",
    "\n",
    "with open('data/names_after_processing.csv', 'w',  encoding=\"utf-8\") as fi:\n",
    "    fi.write(string)\n",
    "\n",
    "df = pd.read_csv(\"data/names_after_processing.csv\", names=['hebrew', 'english'],header=None)\n",
    "# print(len(df))\n",
    "wierd_charachters =  ['װ','í', 'á', 'ó', 'č',  'š', 'ş',  'ł',  'ø', 'é', 'ő', 'î', 'è', 'ć', 'ž', 'ö', 'ň', 'ẓ', 'ü', 'ť', 'ý', 'ã', 'ë', 'ī', 'ḥ', 'ă', 'ñ', 'ä', '־', 'ʻ', 'ğ', 'ï', 'ģ', 'ń', 'ç', 'ū', 'ú', 'ļ', 'ṭ', 'å', 'ḳ', '̈', '1', 'ĥ', 'ð', 'ě', 'ō', 'ô', 'ė', 'ò', 'ř', 'ț',  'ą', 'ß', 'đ', '\\xa0', ':', 'æ', 'ê', 'â', 'ı', 'ù', 'à', 'ś', '︠', '︡',  'ż',  'ţ', 'þ', 'ľ',  'ā', '0', 'ų', 'ĭ', '8', 'ē', 'ņ', 'ÿ', 'ַ', 'θ', 'ε', 'δ', 'ω', 'ρ', 'α', 'ό', 'π', 'υ', 'λ', 'ς', '3', 'õ', 'م', 'ا', 'ل', 'ي', 'ن', '،', 'ׂ', '$', '5', '2', '室', '田', '一', '雄', '′', 'س', 'ف', 'ầ', 'ů', '͡', 'ŏ', 'ả', 'ź', 'ì', '̣', 'ب', 'ر', 'و', 'ױ', '_', 'ə', 'û', 'ŭ', 'د', '9', 'ṣ', 'ḵ', '6', 'أ', 'ك', 'گ', 'ی', '!', 'ت',  'ď', '4', 'ŵ', 'ķ', 'ش', 'ứ', '7', 'œ', 'ũ', 'ű', 'ụ', 'ه', 'ạ', 'ح', 'ذ', 'τ', 'ά', 'ι', 'ṇ','ę', '׳', 'ׂ', '̇', '’', '.', '־', '/', 'ִ', 'ְ', 'ּ', 'ș', 'ʼ', 'ֶ', 'ַ', 'ʾ', '室', '田', '一', '雄', 'ײ', '‘', 'ʹ', 'ׁ', ':', '[', 'س', 'ي', 'ف', 'ر', 'ت', 'ֹ', 'ا', 'ن', 'ṿ', 'ָ', 'ֵ', '!', 'ʿ', '_', 'أ', 'ب', 'و', '`', '1', '0', '8', 'ش', 'ل', 'κ', 'ӏ', 'ׄ', 'م', 'د', '2', 'τ', 'ά', 'ι', 'θ', 'ε', 'ο', 'δ', 'ω', 'ρ', 'α', 'ό', 'π', 'υ', 'λ', 'ς', '\\xa0', '،', '״', 'ױ', '5', 'ك']\n",
    "\n",
    "df.drop(df[(df['english'] == 'i')|\n",
    "(df['english'] == 'ii')|(df['english'] == 'iii')\n",
    "|(df['english'] == 'iv')|(df['english'] == 'v')\n",
    "|(df['english'] == 'vi')|(df['english'] == 'vii')\n",
    "|(df['english'] == 'viii')|(df['english'] == 'ix')\n",
    "|(df['english'] == 'x')|(df['english'] == 'xi')\n",
    "|(df['english'] == 'xii')|(df['english'] == 'xiii')\n",
    "|(df['english'] == 'prince')|(df['english'] == 'princess')\n",
    "|(df['english'] == 'dutchess')|(df['english'] == 'duke')\n",
    "|(df['english'] == 'mr')|(df['english'] == 'ms')|(df['english'] == 'mrs')\n",
    "|(df['english'] == 'the')|(df['english'] == 'for')|(df['english'] == 'of')\n",
    "|(df['english'] == 'king')|(df['english'] == 'queen')|(df['english'] == 'is')\n",
    "|(df['english'].str.contains('^[a-z][.]', regex =True))|(df['english'].str.contains('^[א-ת][.]', regex =True))\n",
    "|(df['english'].str.contains('^[a-z]$', regex =True))|(df['english'].str.contains('^[א-ת]$', regex =True))\n",
    "].index, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "for char in wierd_charachters:\n",
    "    df.drop(df[(df['english'].str.contains(char, regex=False))|(df['hebrew'].str.contains(char, regex=False)) ].index, inplace=True)\n",
    "    \n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "df.drop(df.loc[df['english'].str.contains('[א-ת]', regex=True)].index, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, how='any',  inplace=True)\n",
    "\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv('names_final.csv', index = False)\n",
    "print(len(df))\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_validate_test_split(dataframe, train_percent=.8, validate_percent=.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(dataframe.index)\n",
    "   \n",
    "    m = len(dataframe.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    print(max(perm))\n",
    "    \n",
    "    train = dataframe.iloc[perm[:train_end]]\n",
    "    validate = dataframe.iloc[perm[train_end:validate_end]]\n",
    "    test = dataframe.iloc[perm[validate_end:]]\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "labeled-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87563\n",
      "Train size 78262, Dev size 0, Test size 8710 vocab english size 30 vocab hebrew size 34\n"
     ]
    }
   ],
   "source": [
    "MAX_LABEL_LENGTH = 12\n",
    "class Vocab:\n",
    "    def __init__(self, is_hebrew):\n",
    "        self.char2id = {}\n",
    "        self.id2char = {}\n",
    "        if is_hebrew:\n",
    "            self.n_chars = 2        \n",
    "            self.char2id['^'] = 0\n",
    "            self.char2id['$'] = 1\n",
    "            self.id2char[0] = '^'\n",
    "            self.id2char[1] = '$'\n",
    "        else:\n",
    "            self.n_chars = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def index_text(self, text):\n",
    "        indexes = [self.index_char(c) for c in text]\n",
    "        return indexes\n",
    "    \n",
    "    def index_char(self, c):\n",
    "        if c not in self.char2id:\n",
    "            self.char2id[c] = self.n_chars\n",
    "            self.id2char[self.n_chars] = c\n",
    "            self.n_chars += 1\n",
    "        return self.char2id[c]\n",
    "            \n",
    "            \n",
    "def load_data(data, vocab_english, vocab_hebrew):\n",
    "    data_sequences = []\n",
    "    max_word = 0\n",
    "    num_more_than_20 = 0 \n",
    "    max_word_value = None\n",
    "    for text in data.itertuples():\n",
    "        indexes_english = vocab_english.index_text(text.english)\n",
    "        indexes_hebrew = vocab_hebrew.index_text(text.hebrew)\n",
    "        if len(indexes_hebrew) > max_word:\n",
    "            max_word = len(indexes_hebrew)\n",
    "            max_word_value = indexes_hebrew\n",
    "        if len(indexes_hebrew) <= MAX_LABEL_LENGTH:\n",
    "            for i in range(len(indexes_hebrew),MAX_LABEL_LENGTH, 1):\n",
    "                indexes_hebrew.append(1)\n",
    "            data_sequences.append((indexes_english, indexes_hebrew))\n",
    "\n",
    "    return data_sequences\n",
    "\n",
    "vocab_english = Vocab(False)\n",
    "vocab_hebrew = Vocab(True)\n",
    "train_set, validation_set, test_set = train_validate_test_split(dataframe=df, train_percent=0.9,validate_percent=0.0)\n",
    "\n",
    "train_data = load_data(train_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "dev_data = load_data(validation_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "test_data = load_data(test_set,vocab_english, vocab_hebrew)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Train size {len(train_data)}, Dev size {len(dev_data)}, Test size {len(test_data)} vocab english size {vocab_english.n_chars} vocab hebrew size {vocab_hebrew.n_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historic-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_of_layers):\n",
    "        super(GRU_encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_of_layers = num_of_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size).cuda()\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=num_of_layers).cuda()\n",
    "        \n",
    "    def forward(self, input_letter, hidden):\n",
    "        output = self.embedding(input_letter)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_of_layers, 1, self.hidden_size).cuda()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recovered-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GRU_decoder(nn.Module):\n",
    "    def __init__(self, ouput_size, hidden_size, embedding_size, num_of_layers):\n",
    "        super(GRU_decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_of_layers = num_of_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(ouput_size, embedding_size).cuda()\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers=num_of_layers).cuda()\n",
    "        \n",
    "        self.linear_attention = nn.Linear(2 * hidden_size, hidden_size).cuda()\n",
    "        \n",
    "        self.linear_context = nn.Linear(2 * hidden_size, hidden_size).cuda()\n",
    "        \n",
    "        self.v = nn.Parameter(torch.FloatTensor(hidden_size, 1)).cuda()\n",
    "        \n",
    "        self.v = nn.init.xavier_normal_(self.v)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, ouput_size).cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, output_letter, hidden, encoder_outputs):\n",
    "        output = self.embedding(output_letter)\n",
    "        \n",
    "#         print(\"hidden\" + str(hidden.size()))\n",
    "#         print(encoder_outputs.size())\n",
    "    \n",
    "        weighted_encoder_outputs = torch.clone(encoder_outputs)\n",
    "        alphas = torch.zeros(len(encoder_outputs), self.num_of_layers).cuda()\n",
    "#         print(hidden)\n",
    "#         print(encoder_outputs)\n",
    "        \n",
    "        for i, encoder_output in enumerate(encoder_outputs):\n",
    "            encoder_output = encoder_output.reshape(1,1,-1)\n",
    "#             print(\"encoder_outputs\" + str(encoder_output.size()))\n",
    "#             print().size())\n",
    "#             print(hidden[0][0].size())\n",
    "#             print(encoder_output[0][0].size())\n",
    "#             print((torch.cat((hidden[0][0], encoder_output[0][0]))))\n",
    "            alphas_layers = torch.zeros(self.num_of_layers).cuda()\n",
    "            \n",
    "            for j in range(self.num_of_layers):\n",
    "#                 print(torch.cat((hidden[j][0], encoder_output[0][0])))\n",
    "                w = torch.tanh(self.linear_attention(torch.cat((hidden[j][0], encoder_output[0][0]))))\n",
    "#                 print(w)\n",
    "                w = w.reshape(self.hidden_size, 1)\n",
    "            \n",
    "            \n",
    "#                 print(self.v)\n",
    "#                 print(w)\n",
    "                score = torch.mm(self.v.T, w)\n",
    "#                 print(score)\n",
    "                \n",
    "                alphas_layers[j] = score\n",
    "                \n",
    "#             alphas.append\n",
    "            alphas[i] = alphas_layers\n",
    "#         print(alphas)\n",
    "        \n",
    "#             alphas = 1              \n",
    "#             weighted_encoder_outputs.append(encoder_output)\n",
    "#         print(alphas.size())\n",
    "#         print(encoder_outputs[0][0][0])\n",
    "#         print(alphas)\n",
    "        alphas = F.softmax(alphas, dim=0)\n",
    "#         print(alphas)\n",
    "        alphas = alphas.reshape(len(alphas), self.num_of_layers, 1)\n",
    "        \n",
    "#         print(alphas)\n",
    "#         print(encoder_outputs.size())\n",
    "#         print(encoder_outputs)\n",
    "        weighted_encoder_outputs =  torch.mul(encoder_outputs, alphas)\n",
    "#         print(weighted_encoder_outputs)\n",
    "#         print(weighted_encoder_outputs[0][0][0])\n",
    "#         print(alphas[0])\n",
    "#         print(context)\n",
    "#         print(weighted_encoder_outputs.size())\n",
    "#         print(weighted_encoder_outputs[0].size())\n",
    "#         print(alphas)\n",
    "        context = torch.sum(weighted_encoder_outputs, dim=0, keepdim=True)\n",
    "#         print(weighted_encoder_outputs[0].size())\n",
    "#         print(context)\n",
    "        \n",
    "        \n",
    "#         weights_i = F.softmax(score(hidden, encoder_outputs[i]))\n",
    "        \n",
    "#         score(hidden, outputs[i]) = v.T* F.tanh(self.linear_attention(hidden+outputs[i]))\n",
    "        \n",
    "#         context = sum(weights*outputs)\n",
    "        context = context.reshape(self.num_of_layers, 1, self.hidden_size)\n",
    "#         print(context.size())\n",
    "#         print(hidden.size())\n",
    "#         print(hidden[1][0])\n",
    "        hidden_concatenated = torch.zeros(self.num_of_layers, 1, self.hidden_size*2).cuda()\n",
    "        for k in range(self.num_of_layers):\n",
    "#             print(hidden[k][0])\n",
    "#             print(context[k][0])\n",
    "#             print(torch.cat((hidden[k][0], context[k][0])))\n",
    "            hidden_concatenated[k][0] = torch.cat((hidden[k][0], context[k][0]))\n",
    "#         print(hidden_concatenated.size())\n",
    "#         hidden = hidden.reshape(self.num_of_layers,1,2*self.hidden_size)\n",
    "        \n",
    "        hidden = self.linear_context(hidden_concatenated)\n",
    "#         print(hidden.size())\n",
    "#         hidden = F.relu(hidden)\n",
    "#         print(output.size())\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "#         print(output.size())\n",
    "#         print(hidden.size())\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        \n",
    "#         output = F.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "great-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class seq2seq_GRU(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_size, output_size, hidden_size, embedding_size):\n",
    "#         super(seq2seq_GRU, self).__init__()\n",
    "        \n",
    "#         self.encoder = GRU_encoder(input_size=input_size, hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "                \n",
    "#         self.decoder = GRU_decoder(output_size=output_size, hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "        \n",
    "        \n",
    "#     def forward(self, input_word):\n",
    "#         output, hidden = self.encoder(input_word)\n",
    "        \n",
    "#         output = self.decoder(, hidden)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brilliant-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = GRU_encoder(vocab_english.n_chars, 300, 64, 2)\n",
    "decoder_model = GRU_decoder(vocab_hebrew.n_chars, 300, 64, 2)\n",
    "\n",
    "encoder_optimizer = optim.Adam(params=encoder_model.parameters(), lr=0.0001)\n",
    "decoder_optimizer = optim.Adam(params=decoder_model.parameters(), lr=0.0001)\n",
    "\n",
    "critertion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heavy-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(n_epochs=4, print_every=5000):\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(1, n_epochs + 1):\n",
    "\n",
    "        for counter, (english_name, hebrew_name) in enumerate(train_data):\n",
    "            encoder_hidden = encoder_model.initHidden()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "    #         english_name_tensor = torch.LongTensor(english_name).cuda()\n",
    "            encoder_outputs = torch.zeros(len(english_name), 1, encoder_model.hidden_size).cuda()\n",
    "    #         hebrew_name_tensor = torch.LongTensor(hebrew_name).cuda()\n",
    "    \n",
    "            for i, letter in enumerate(english_name):\n",
    "                letter_tensor = torch.tensor([[letter]]).cuda()\n",
    "                encoder_output, encoder_hidden = encoder_model(letter_tensor, encoder_hidden)\n",
    "                \n",
    "                encoder_outputs[i] = encoder_output\n",
    "            \n",
    "            decoder_input = torch.tensor([[0]]).cuda()\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "            transliration_loss = 0 \n",
    "            \n",
    "            \n",
    "            for i in range(len(hebrew_name)):\n",
    "#                 print(i)\n",
    "                decoder_output, decoder_hidden = decoder_model(decoder_input, decoder_hidden, encoder_outputs)\n",
    "#                 print(i)\n",
    "                \n",
    "                \n",
    "                _, top_indexes = decoder_output.topk(1)\n",
    "\n",
    "                decoder_input = top_indexes.squeeze(0).detach()\n",
    "#                 return\n",
    "                \n",
    "#                 print(decoder_hidden.size())\n",
    "                transliration_loss += critertion(decoder_output.reshape(decoder_output.shape[1], decoder_output.shape[2]), torch.tensor([hebrew_name[i]]).cuda())\n",
    "                if decoder_input.item() == [1]:\n",
    "                    break\n",
    "#             return \n",
    "            transliration_loss.backward()\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            loss += transliration_loss.item() / len(hebrew_name)\n",
    "\n",
    "\n",
    "            if counter % print_every == 0 and counter != 0:\n",
    "                loss = loss / print_every\n",
    "                print('Epoch %d,/%d, Current Loss = %.4f' % (e, counter,loss))\n",
    "                loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,/1000, Current Loss = 1.7239\n",
      "Epoch 1,/2000, Current Loss = 1.5020\n",
      "Epoch 1,/3000, Current Loss = 1.4271\n",
      "Epoch 1,/4000, Current Loss = 1.2490\n",
      "Epoch 1,/5000, Current Loss = 1.1587\n",
      "Epoch 1,/6000, Current Loss = 1.0456\n",
      "Epoch 1,/7000, Current Loss = 1.0567\n",
      "Epoch 1,/8000, Current Loss = 0.9858\n",
      "Epoch 1,/9000, Current Loss = 0.9516\n",
      "Epoch 1,/10000, Current Loss = 0.9636\n",
      "Epoch 1,/11000, Current Loss = 0.9730\n",
      "Epoch 1,/12000, Current Loss = 0.9223\n",
      "Epoch 1,/13000, Current Loss = 0.8984\n",
      "Epoch 1,/14000, Current Loss = 0.8971\n",
      "Epoch 1,/15000, Current Loss = 0.8744\n",
      "Epoch 1,/16000, Current Loss = 0.9056\n",
      "Epoch 1,/17000, Current Loss = 0.8670\n",
      "Epoch 1,/18000, Current Loss = 0.8383\n",
      "Epoch 1,/19000, Current Loss = 0.8431\n",
      "Epoch 1,/20000, Current Loss = 0.8291\n",
      "Epoch 1,/21000, Current Loss = 0.8189\n",
      "Epoch 1,/22000, Current Loss = 0.8006\n",
      "Epoch 1,/23000, Current Loss = 0.7987\n",
      "Epoch 1,/24000, Current Loss = 0.8021\n",
      "Epoch 1,/25000, Current Loss = 0.8566\n",
      "Epoch 1,/26000, Current Loss = 0.8425\n",
      "Epoch 1,/27000, Current Loss = 0.8439\n",
      "Epoch 1,/28000, Current Loss = 0.8084\n",
      "Epoch 1,/29000, Current Loss = 0.8324\n",
      "Epoch 1,/30000, Current Loss = 0.8159\n",
      "Epoch 1,/31000, Current Loss = 0.8497\n",
      "Epoch 1,/32000, Current Loss = 0.8437\n",
      "Epoch 1,/33000, Current Loss = 0.8149\n",
      "Epoch 1,/34000, Current Loss = 0.7975\n",
      "Epoch 1,/35000, Current Loss = 0.7987\n",
      "Epoch 1,/36000, Current Loss = 0.7531\n",
      "Epoch 1,/37000, Current Loss = 0.7653\n",
      "Epoch 1,/38000, Current Loss = 0.7697\n",
      "Epoch 1,/39000, Current Loss = 0.7768\n",
      "Epoch 1,/40000, Current Loss = 0.7835\n",
      "Epoch 1,/41000, Current Loss = 0.7860\n",
      "Epoch 1,/42000, Current Loss = 0.7883\n",
      "Epoch 1,/43000, Current Loss = 0.7506\n",
      "Epoch 1,/44000, Current Loss = 0.7752\n",
      "Epoch 1,/45000, Current Loss = 0.7642\n",
      "Epoch 1,/46000, Current Loss = 0.8250\n",
      "Epoch 1,/47000, Current Loss = 0.7922\n",
      "Epoch 1,/48000, Current Loss = 0.7554\n",
      "Epoch 1,/49000, Current Loss = 0.7631\n",
      "Epoch 1,/50000, Current Loss = 0.7473\n",
      "Epoch 1,/51000, Current Loss = 0.7727\n",
      "Epoch 1,/52000, Current Loss = 0.8065\n",
      "Epoch 1,/53000, Current Loss = 0.7144\n",
      "Epoch 1,/54000, Current Loss = 0.7876\n",
      "Epoch 1,/55000, Current Loss = 0.7425\n",
      "Epoch 1,/56000, Current Loss = 0.7440\n",
      "Epoch 1,/57000, Current Loss = 0.7410\n",
      "Epoch 1,/58000, Current Loss = 0.7650\n",
      "Epoch 1,/59000, Current Loss = 0.7184\n",
      "Epoch 1,/60000, Current Loss = 0.7501\n",
      "Epoch 1,/61000, Current Loss = 0.7439\n",
      "Epoch 1,/62000, Current Loss = 0.7332\n",
      "Epoch 1,/63000, Current Loss = 0.7571\n",
      "Epoch 1,/64000, Current Loss = 0.7327\n",
      "Epoch 1,/65000, Current Loss = 0.6962\n",
      "Epoch 1,/66000, Current Loss = 0.7329\n",
      "Epoch 1,/67000, Current Loss = 0.7539\n",
      "Epoch 1,/68000, Current Loss = 0.7324\n",
      "Epoch 1,/69000, Current Loss = 0.7163\n",
      "Epoch 1,/70000, Current Loss = 0.7260\n",
      "Epoch 1,/71000, Current Loss = 0.7113\n",
      "Epoch 1,/72000, Current Loss = 0.7312\n",
      "Epoch 1,/73000, Current Loss = 0.7408\n",
      "Epoch 1,/74000, Current Loss = 0.7193\n",
      "Epoch 1,/75000, Current Loss = 0.6934\n",
      "Epoch 1,/76000, Current Loss = 0.7003\n",
      "Epoch 1,/77000, Current Loss = 0.6980\n",
      "Epoch 1,/78000, Current Loss = 0.7284\n",
      "Epoch 2,/1000, Current Loss = 0.9328\n",
      "Epoch 2,/2000, Current Loss = 0.6927\n",
      "Epoch 2,/3000, Current Loss = 0.7200\n",
      "Epoch 2,/4000, Current Loss = 0.7070\n",
      "Epoch 2,/5000, Current Loss = 0.7488\n",
      "Epoch 2,/6000, Current Loss = 0.6969\n",
      "Epoch 2,/7000, Current Loss = 0.7588\n",
      "Epoch 2,/8000, Current Loss = 0.7087\n",
      "Epoch 2,/9000, Current Loss = 0.7026\n",
      "Epoch 2,/10000, Current Loss = 0.7404\n",
      "Epoch 2,/11000, Current Loss = 0.7818\n",
      "Epoch 2,/12000, Current Loss = 0.7203\n",
      "Epoch 2,/13000, Current Loss = 0.7211\n",
      "Epoch 2,/14000, Current Loss = 0.7212\n",
      "Epoch 2,/15000, Current Loss = 0.7057\n",
      "Epoch 2,/16000, Current Loss = 0.7457\n",
      "Epoch 2,/17000, Current Loss = 0.7129\n",
      "Epoch 2,/18000, Current Loss = 0.7007\n",
      "Epoch 2,/19000, Current Loss = 0.7052\n",
      "Epoch 2,/20000, Current Loss = 0.6976\n",
      "Epoch 2,/21000, Current Loss = 0.6943\n",
      "Epoch 2,/22000, Current Loss = 0.6799\n",
      "Epoch 2,/23000, Current Loss = 0.6787\n",
      "Epoch 2,/24000, Current Loss = 0.6856\n",
      "Epoch 2,/25000, Current Loss = 0.7525\n",
      "Epoch 2,/26000, Current Loss = 0.7292\n",
      "Epoch 2,/27000, Current Loss = 0.7311\n",
      "Epoch 2,/28000, Current Loss = 0.7154\n",
      "Epoch 2,/29000, Current Loss = 0.7218\n",
      "Epoch 2,/30000, Current Loss = 0.7282\n",
      "Epoch 2,/31000, Current Loss = 0.7547\n",
      "Epoch 2,/32000, Current Loss = 0.7443\n",
      "Epoch 2,/33000, Current Loss = 0.7298\n",
      "Epoch 2,/34000, Current Loss = 0.7137\n",
      "Epoch 2,/35000, Current Loss = 0.7091\n",
      "Epoch 2,/36000, Current Loss = 0.6715\n",
      "Epoch 2,/37000, Current Loss = 0.6825\n",
      "Epoch 2,/38000, Current Loss = 0.6854\n",
      "Epoch 2,/39000, Current Loss = 0.6973\n",
      "Epoch 2,/40000, Current Loss = 0.6986\n",
      "Epoch 2,/41000, Current Loss = 0.7054\n",
      "Epoch 2,/42000, Current Loss = 0.7186\n",
      "Epoch 2,/43000, Current Loss = 0.6767\n",
      "Epoch 2,/44000, Current Loss = 0.6966\n",
      "Epoch 2,/45000, Current Loss = 0.6888\n",
      "Epoch 2,/46000, Current Loss = 0.7482\n",
      "Epoch 2,/47000, Current Loss = 0.7237\n",
      "Epoch 2,/48000, Current Loss = 0.6874\n",
      "Epoch 2,/49000, Current Loss = 0.6899\n",
      "Epoch 2,/50000, Current Loss = 0.6871\n",
      "Epoch 2,/51000, Current Loss = 0.7122\n",
      "Epoch 2,/52000, Current Loss = 0.7359\n",
      "Epoch 2,/53000, Current Loss = 0.6474\n",
      "Epoch 2,/54000, Current Loss = 0.7282\n",
      "Epoch 2,/55000, Current Loss = 0.6826\n",
      "Epoch 2,/56000, Current Loss = 0.6806\n",
      "Epoch 2,/57000, Current Loss = 0.6809\n",
      "Epoch 2,/58000, Current Loss = 0.7027\n",
      "Epoch 2,/59000, Current Loss = 0.6500\n",
      "Epoch 2,/60000, Current Loss = 0.6916\n",
      "Epoch 2,/61000, Current Loss = 0.6756\n",
      "Epoch 2,/62000, Current Loss = 0.6794\n",
      "Epoch 2,/63000, Current Loss = 0.6961\n",
      "Epoch 2,/64000, Current Loss = 0.6765\n",
      "Epoch 2,/65000, Current Loss = 0.6425\n",
      "Epoch 2,/66000, Current Loss = 0.6741\n",
      "Epoch 2,/67000, Current Loss = 0.6973\n",
      "Epoch 2,/68000, Current Loss = 0.6773\n",
      "Epoch 2,/69000, Current Loss = 0.6682\n",
      "Epoch 2,/70000, Current Loss = 0.6746\n",
      "Epoch 2,/71000, Current Loss = 0.6522\n",
      "Epoch 2,/72000, Current Loss = 0.6784\n",
      "Epoch 2,/73000, Current Loss = 0.6824\n",
      "Epoch 2,/74000, Current Loss = 0.6613\n",
      "Epoch 2,/75000, Current Loss = 0.6444\n",
      "Epoch 2,/76000, Current Loss = 0.6529\n",
      "Epoch 2,/77000, Current Loss = 0.6446\n",
      "Epoch 2,/78000, Current Loss = 0.6789\n",
      "Epoch 3,/1000, Current Loss = 0.8673\n",
      "Epoch 3,/2000, Current Loss = 0.6480\n",
      "Epoch 3,/3000, Current Loss = 0.6656\n",
      "Epoch 3,/4000, Current Loss = 0.6552\n",
      "Epoch 3,/5000, Current Loss = 0.6999\n",
      "Epoch 3,/6000, Current Loss = 0.6535\n",
      "Epoch 3,/7000, Current Loss = 0.7057\n",
      "Epoch 3,/8000, Current Loss = 0.6642\n",
      "Epoch 3,/9000, Current Loss = 0.6524\n",
      "Epoch 3,/10000, Current Loss = 0.6942\n",
      "Epoch 3,/11000, Current Loss = 0.7391\n",
      "Epoch 3,/12000, Current Loss = 0.6772\n",
      "Epoch 3,/13000, Current Loss = 0.6741\n",
      "Epoch 3,/14000, Current Loss = 0.6784\n",
      "Epoch 3,/15000, Current Loss = 0.6626\n",
      "Epoch 3,/16000, Current Loss = 0.7018\n",
      "Epoch 3,/17000, Current Loss = 0.6634\n",
      "Epoch 3,/18000, Current Loss = 0.6606\n",
      "Epoch 3,/19000, Current Loss = 0.6605\n",
      "Epoch 3,/20000, Current Loss = 0.6575\n",
      "Epoch 3,/21000, Current Loss = 0.6510\n",
      "Epoch 3,/22000, Current Loss = 0.6421\n",
      "Epoch 3,/23000, Current Loss = 0.6393\n",
      "Epoch 3,/24000, Current Loss = 0.6451\n",
      "Epoch 3,/25000, Current Loss = 0.7115\n",
      "Epoch 3,/26000, Current Loss = 0.6861\n",
      "Epoch 3,/27000, Current Loss = 0.6892\n",
      "Epoch 3,/28000, Current Loss = 0.6778\n",
      "Epoch 3,/29000, Current Loss = 0.6762\n",
      "Epoch 3,/30000, Current Loss = 0.6873\n",
      "Epoch 3,/31000, Current Loss = 0.7152\n",
      "Epoch 3,/32000, Current Loss = 0.7001\n",
      "Epoch 3,/33000, Current Loss = 0.6896\n",
      "Epoch 3,/34000, Current Loss = 0.6724\n",
      "Epoch 3,/35000, Current Loss = 0.6662\n",
      "Epoch 3,/36000, Current Loss = 0.6329\n",
      "Epoch 3,/37000, Current Loss = 0.6479\n",
      "Epoch 3,/38000, Current Loss = 0.6479\n",
      "Epoch 3,/39000, Current Loss = 0.6554\n",
      "Epoch 3,/40000, Current Loss = 0.6612\n",
      "Epoch 3,/41000, Current Loss = 0.6702\n",
      "Epoch 3,/42000, Current Loss = 0.6810\n",
      "Epoch 3,/43000, Current Loss = 0.6451\n",
      "Epoch 3,/44000, Current Loss = 0.6526\n",
      "Epoch 3,/45000, Current Loss = 0.6561\n",
      "Epoch 3,/46000, Current Loss = 0.7107\n",
      "Epoch 3,/47000, Current Loss = 0.6876\n",
      "Epoch 3,/48000, Current Loss = 0.6526\n",
      "Epoch 3,/49000, Current Loss = 0.6539\n",
      "Epoch 3,/50000, Current Loss = 0.6547\n",
      "Epoch 3,/51000, Current Loss = 0.6709\n",
      "Epoch 3,/52000, Current Loss = 0.6924\n",
      "Epoch 3,/53000, Current Loss = 0.6125\n",
      "Epoch 3,/54000, Current Loss = 0.6897\n",
      "Epoch 3,/55000, Current Loss = 0.6507\n",
      "Epoch 3,/56000, Current Loss = 0.6419\n",
      "Epoch 3,/57000, Current Loss = 0.6466\n",
      "Epoch 3,/58000, Current Loss = 0.6626\n",
      "Epoch 3,/59000, Current Loss = 0.6144\n",
      "Epoch 3,/60000, Current Loss = 0.6541\n",
      "Epoch 3,/61000, Current Loss = 0.6398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3,/62000, Current Loss = 0.6501\n",
      "Epoch 3,/63000, Current Loss = 0.6614\n",
      "Epoch 3,/64000, Current Loss = 0.6418\n",
      "Epoch 3,/65000, Current Loss = 0.6100\n",
      "Epoch 3,/66000, Current Loss = 0.6367\n",
      "Epoch 3,/67000, Current Loss = 0.6578\n",
      "Epoch 3,/68000, Current Loss = 0.6430\n",
      "Epoch 3,/69000, Current Loss = 0.6332\n",
      "Epoch 3,/70000, Current Loss = 0.6453\n",
      "Epoch 3,/71000, Current Loss = 0.6224\n",
      "Epoch 3,/72000, Current Loss = 0.6395\n",
      "Epoch 3,/73000, Current Loss = 0.6456\n",
      "Epoch 3,/74000, Current Loss = 0.6260\n",
      "Epoch 3,/75000, Current Loss = 0.6063\n",
      "Epoch 3,/76000, Current Loss = 0.6145\n",
      "Epoch 3,/77000, Current Loss = 0.6115\n",
      "Epoch 3,/78000, Current Loss = 0.6472\n",
      "Epoch 4,/1000, Current Loss = 0.8258\n",
      "Epoch 4,/2000, Current Loss = 0.6184\n",
      "Epoch 4,/3000, Current Loss = 0.6259\n",
      "Epoch 4,/4000, Current Loss = 0.6246\n",
      "Epoch 4,/5000, Current Loss = 0.6707\n",
      "Epoch 4,/6000, Current Loss = 0.6213\n",
      "Epoch 4,/7000, Current Loss = 0.6765\n",
      "Epoch 4,/8000, Current Loss = 0.6305\n",
      "Epoch 4,/9000, Current Loss = 0.6177\n",
      "Epoch 4,/10000, Current Loss = 0.6629\n",
      "Epoch 4,/11000, Current Loss = 0.7037\n",
      "Epoch 4,/12000, Current Loss = 0.6440\n",
      "Epoch 4,/13000, Current Loss = 0.6435\n",
      "Epoch 4,/14000, Current Loss = 0.6462\n",
      "Epoch 4,/15000, Current Loss = 0.6252\n",
      "Epoch 4,/16000, Current Loss = 0.6668\n",
      "Epoch 4,/17000, Current Loss = 0.6290\n",
      "Epoch 4,/18000, Current Loss = 0.6289\n",
      "Epoch 4,/19000, Current Loss = 0.6300\n",
      "Epoch 4,/20000, Current Loss = 0.6222\n",
      "Epoch 4,/21000, Current Loss = 0.6166\n",
      "Epoch 4,/22000, Current Loss = 0.6144\n",
      "Epoch 4,/23000, Current Loss = 0.6113\n",
      "Epoch 4,/24000, Current Loss = 0.6185\n",
      "Epoch 4,/25000, Current Loss = 0.6809\n",
      "Epoch 4,/26000, Current Loss = 0.6588\n",
      "Epoch 4,/27000, Current Loss = 0.6566\n",
      "Epoch 4,/28000, Current Loss = 0.6456\n",
      "Epoch 4,/29000, Current Loss = 0.6463\n",
      "Epoch 4,/30000, Current Loss = 0.6516\n",
      "Epoch 4,/31000, Current Loss = 0.6828\n",
      "Epoch 4,/32000, Current Loss = 0.6713\n",
      "Epoch 4,/33000, Current Loss = 0.6626\n",
      "Epoch 4,/34000, Current Loss = 0.6440\n",
      "Epoch 4,/35000, Current Loss = 0.6338\n",
      "Epoch 4,/36000, Current Loss = 0.6034\n",
      "Epoch 4,/37000, Current Loss = 0.6203\n",
      "Epoch 4,/38000, Current Loss = 0.6196\n",
      "Epoch 4,/39000, Current Loss = 0.6281\n",
      "Epoch 4,/40000, Current Loss = 0.6324\n",
      "Epoch 4,/41000, Current Loss = 0.6393\n",
      "Epoch 4,/42000, Current Loss = 0.6456\n",
      "Epoch 4,/43000, Current Loss = 0.6186\n",
      "Epoch 4,/44000, Current Loss = 0.6223\n",
      "Epoch 4,/45000, Current Loss = 0.6256\n",
      "Epoch 4,/46000, Current Loss = 0.6766\n",
      "Epoch 4,/47000, Current Loss = 0.6557\n",
      "Epoch 4,/48000, Current Loss = 0.6244\n",
      "Epoch 4,/49000, Current Loss = 0.6263\n",
      "Epoch 4,/50000, Current Loss = 0.6284\n",
      "Epoch 4,/51000, Current Loss = 0.6402\n",
      "Epoch 4,/52000, Current Loss = 0.6624\n",
      "Epoch 4,/53000, Current Loss = 0.5829\n",
      "Epoch 4,/54000, Current Loss = 0.6613\n",
      "Epoch 4,/55000, Current Loss = 0.6215\n",
      "Epoch 4,/56000, Current Loss = 0.6167\n",
      "Epoch 4,/57000, Current Loss = 0.6130\n",
      "Epoch 4,/58000, Current Loss = 0.6307\n",
      "Epoch 4,/59000, Current Loss = 0.5808\n",
      "Epoch 4,/60000, Current Loss = 0.6246\n",
      "Epoch 4,/61000, Current Loss = 0.6101\n",
      "Epoch 4,/62000, Current Loss = 0.6177\n",
      "Epoch 4,/63000, Current Loss = 0.6357\n",
      "Epoch 4,/64000, Current Loss = 0.6120\n",
      "Epoch 4,/65000, Current Loss = 0.5879\n",
      "Epoch 4,/66000, Current Loss = 0.6064\n",
      "Epoch 4,/67000, Current Loss = 0.6195\n",
      "Epoch 4,/68000, Current Loss = 0.6144\n",
      "Epoch 4,/69000, Current Loss = 0.6023\n",
      "Epoch 4,/70000, Current Loss = 0.6139\n",
      "Epoch 4,/71000, Current Loss = 0.5952\n",
      "Epoch 4,/72000, Current Loss = 0.6113\n",
      "Epoch 4,/73000, Current Loss = 0.6221\n",
      "Epoch 4,/74000, Current Loss = 0.5946\n",
      "Epoch 4,/75000, Current Loss = 0.5738\n",
      "Epoch 4,/76000, Current Loss = 0.5855\n",
      "Epoch 4,/77000, Current Loss = 0.5833\n",
      "Epoch 4,/78000, Current Loss = 0.6142\n"
     ]
    }
   ],
   "source": [
    "train(print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lovely-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(print_every=5000):\n",
    "    accuracy = 0\n",
    "    num_of_letters = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for counter, (english_name, hebrew_name) in enumerate(test_data):\n",
    "            encoder_hidden = encoder_model.initHidden()\n",
    "\n",
    "           \n",
    "\n",
    "            encoder_outputs = torch.zeros(len(english_name), 1, encoder_model.hidden_size).cuda()\n",
    "    \n",
    "            for i, letter in enumerate(english_name):\n",
    "                letter_tensor = torch.tensor([[letter]]).cuda()\n",
    "                encoder_output, encoder_hidden = encoder_model(letter_tensor, encoder_hidden)\n",
    "                \n",
    "                encoder_outputs[i] = encoder_output\n",
    "            \n",
    "            decoder_input = torch.tensor([[0]]).cuda()\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "            guessed_word = []\n",
    "            \n",
    "            for i in range(len(hebrew_name)):\n",
    "                decoder_output, decoder_hidden = decoder_model(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                \n",
    "                \n",
    "                _, top_indexes = decoder_output.topk(1)\n",
    "#                 print(decoder_output.size())\n",
    "                decoder_input = top_indexes.squeeze(0).detach()\n",
    "#                 print(torch.max(decoder_output,dim=2).indices.size())\n",
    "                guessed_word.append(torch.max(decoder_output,dim=2).indices[0])\n",
    "                \n",
    "#                 print(decoder_output.size())\n",
    "#                 if decoder_input.item() == [1]:\n",
    "#                     break\n",
    "            \n",
    "            for i in range(MAX_LABEL_LENGTH):\n",
    "                if guessed_word[i] == hebrew_name[i] and guessed_word[i] == 1:\n",
    "                    break\n",
    "                \n",
    "                num_of_letters += 1\n",
    "                if guessed_word[i] == hebrew_name[i]:\n",
    "                    accuracy += 1\n",
    "\n",
    "        \n",
    "            if counter % print_every == 0 and counter != 0:\n",
    "                print(counter)\n",
    "    print(accuracy / num_of_letters)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "streaming-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "0.6177784595325293\n"
     ]
    }
   ],
   "source": [
    "test(print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "checked-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:48:11\n"
     ]
    }
   ],
   "source": [
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
